import av
import streamlit as st
import time
from PIL import Image
from streamlit_webrtc import WebRtcMode, VideoProcessorBase, webrtc_streamer

from services.deepface_service import analyze_emotion
from services.gemini_service import (
    get_gemini_api_key,
    init_gemini,
    create_emotion_intro,
)
from services.emotion_agent_service import (
    generate_advice_with_memory_from_result,
)

from services.tts_service import text_to_speech_file, estimate_speech_duration, cleanup_audio_file


class EmotionVideoProcessor(VideoProcessorBase):
    """
    Video processor d√πng cho streamlit-webrtc.
    Ch·ªâ gi·ªØ frame m·ªõi nh·∫•t t·ª´ camera, ƒë·ªÉ thread ch√≠nh quy·∫øt ƒë·ªãnh khi n√†o ch·ª•p.
    """

    def __init__(self):
        # Frame BGR m·ªõi nh·∫•t t·ª´ camera
        self.last_frame_bgr = None
        # ·∫¢nh ƒë√£ ƒë∆∞·ª£c "ch·ª•p" (PIL Image) gi·ªëng nh∆∞ Take photo
        self.captured_image = None
        # K·∫øt qu·∫£ ph√¢n t√≠ch c·∫£m x√∫c g·∫ßn nh·∫•t
        self.last_result = None

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        # L∆∞u frame m·ªõi nh·∫•t, kh√¥ng ph√¢n t√≠ch t·∫°i ƒë√¢y
        img_bgr = frame.to_ndarray(format="bgr24")
        self.last_frame_bgr = img_bgr
        return frame


def render_camera_auto(interval_seconds: int = 15):
    """
    Giao di·ªán v√† logic cho ch·∫ø ƒë·ªô Camera auto.
    Sequential flow: Detect emotion ‚Üí Call Gemini ‚Üí Show response ‚Üí Detect ti·∫øp
    """
    st.subheader("ü§ñ Tr·ª£ l√Ω c·∫£m x√∫c AI - Ch·∫ø ƒë·ªô t·ª± ƒë·ªông")
    st.write(
        "**Quy tr√¨nh:** Detect c·∫£m x√∫c ‚Üí AI ph√¢n t√≠ch v√† ƒë∆∞a l·ªùi ƒë·ªông vi√™n ‚Üí Detect ti·∫øp\n\n"
        "B·∫≠t camera b√™n d∆∞·ªõi, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông detect c·∫£m x√∫c v√† ƒë·ª£i AI tr·∫£ l·ªùi xong m·ªõi detect ti·∫øp."
    )
    
    # Kh·ªüi t·∫°o session state
    if "previous_emotion" not in st.session_state:
        st.session_state.previous_emotion = None
    if "is_gemini_processing" not in st.session_state:
        st.session_state.is_gemini_processing = False
    if "last_gemini_suggestion" not in st.session_state:
        st.session_state.last_gemini_suggestion = None
    if "last_detection_time" not in st.session_state:
        st.session_state.last_detection_time = 0
    if "waiting_for_ai" not in st.session_state:
        st.session_state.waiting_for_ai = False
    if "is_playing_audio" not in st.session_state:
        st.session_state.is_playing_audio = False
    if "current_audio_file" not in st.session_state:
        st.session_state.current_audio_file = None
    
    # Control buttons
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("üîÑ Reset v√† b·∫Øt ƒë·∫ßu l·∫°i"):
            st.session_state.previous_emotion = None
            st.session_state.last_gemini_suggestion = None
            st.session_state.waiting_for_ai = False
            st.session_state.is_gemini_processing = False
            st.session_state.last_detection_time = 0
            st.session_state.force_detect = True
            st.success("‚úÖ ƒê√£ reset! S·∫µn s√†ng detect c·∫£m x√∫c m·ªõi.")
            st.rerun()
    
    with col2:
        if st.button("‚ñ∂Ô∏è Detect c·∫£m x√∫c ngay"):
            st.session_state.waiting_for_ai = False
            st.session_state.last_detection_time = 0
            st.session_state.force_detect = True
            st.rerun()
    
    with col3:
        auto_mode = st.checkbox("üîÑ T·ª± ƒë·ªông detect", value=False, key="auto_detect_mode")
    
    # TTS settings
    tts_enabled = st.checkbox("üîä B·∫≠t ƒë·ªçc text-to-speech", value=True, key="tts_enabled")
    
    # Kh·ªüi t·∫°o force_detect flag
    if "force_detect" not in st.session_state:
        st.session_state.force_detect = False

    webrtc_ctx = webrtc_streamer(
        key=f"emotion-auto-{interval_seconds}",
        mode=WebRtcMode.SENDRECV,
        media_stream_constraints={"video": True, "audio": False},
        video_processor_factory=EmotionVideoProcessor,
    )

    result_placeholder = st.empty()
    chart_placeholder = st.empty()
    suggestion_placeholder = st.empty()
    status_placeholder = st.empty()
    audio_placeholder = st.empty()

    if webrtc_ctx.video_processor is not None:
        processor = webrtc_ctx.video_processor

        # N·∫øu ƒëang ch·ªù AI response, ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ c≈©
        if st.session_state.is_gemini_processing or st.session_state.waiting_for_ai:
            status_placeholder.info("‚è≥ **ƒêang ch·ªù AI tr·∫£ l·ªùi...** Vui l√≤ng ƒë·ª£i.")
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ c≈© n·∫øu c√≥
            if processor.last_result:
                result = processor.last_result
                dominant_emotion = result.get("dominant_emotion")
                emotions = result.get("emotion", {})
                result_placeholder.success(f"**C·∫£m x√∫c ch√≠nh**: {dominant_emotion}")
                if emotions:
                    chart_placeholder.subheader("Chi ti·∫øt c√°c c·∫£m x√∫c")
                    chart_placeholder.bar_chart(emotions)
            if st.session_state.last_gemini_suggestion:
                suggestion_placeholder.markdown(
                    f"### üí¨ G·ª£i √Ω t·ª´ tr·ª£ l√Ω c·∫£m x√∫c\n\n{st.session_state.last_gemini_suggestion}"
                )
        else:
            # Ki·ªÉm tra xem c√≥ n√™n detect kh√¥ng (auto mode, button ƒë∆∞·ª£c nh·∫•n, ho·∫∑c force detect)
            should_detect = (
                st.session_state.force_detect or
                auto_mode or 
                st.session_state.last_detection_time == 0 or
                (time.time() - st.session_state.last_detection_time) > interval_seconds
            )
            
            # Reset force_detect flag sau khi d√πng
            if st.session_state.force_detect:
                st.session_state.force_detect = False
            
            # N·∫øu kh√¥ng ƒëang ch·ªù AI v√† c√≥ frame, lu√¥n capture v√† detect
            if should_detect and processor.last_frame_bgr is not None:
                try:
                    frame_rgb = processor.last_frame_bgr[:, :, ::-1]  # BGR -> RGB
                    image = Image.fromarray(frame_rgb)
                    processor.captured_image = image

                    # Ph√¢n t√≠ch c·∫£m x√∫c
                    with st.spinner("üîç ƒêang ph√¢n t√≠ch c·∫£m x√∫c..."):
                        result = analyze_emotion(image)
                    processor.last_result = result
                    st.session_state.last_detection_time = time.time()
                except Exception as e:
                    st.warning(f"L·ªói khi detect c·∫£m x√∫c: {e}")

        # Hi·ªÉn th·ªã ·∫£nh n·∫øu c√≥
        if processor.captured_image is not None:
            st.image(
                processor.captured_image,
                caption="·∫¢nh ƒë√£ capture",
                use_column_width=True,
            )

        # X·ª≠ l√Ω k·∫øt qu·∫£ v√† g·ªçi Gemini
        if not st.session_state.is_gemini_processing and not st.session_state.waiting_for_ai:
            result = processor.last_result
            if result:
                dominant_emotion = result.get("dominant_emotion")
                emotions = result.get("emotion", {})

                result_placeholder.success(f"**C·∫£m x√∫c ch√≠nh**: {dominant_emotion}")

                if emotions:
                    chart_placeholder.subheader("Chi ti·∫øt c√°c c·∫£m x√∫c")
                    chart_placeholder.bar_chart(emotions)

                # --- G·ªçi Gemini CH·ªà KHI emotion thay ƒë·ªïi ---
                api_key = get_gemini_api_key()
                if not api_key:
                    suggestion_placeholder.warning(
                        "‚ö†Ô∏è **Ch∆∞a t√¨m th·∫•y Gemini API key!**\n\n"
                        "Vui l√≤ng:\n"
                        "1. T·∫°o file `.env` trong th∆∞ m·ª•c g·ªëc v·ªõi n·ªôi dung: `GEMINI_API_KEY=your_key_here`\n"
                        "2. Ho·∫∑c nh·∫≠p API key ·ªü sidebar\n"
                        "3. Ho·∫∑c set bi·∫øn m√¥i tr∆∞·ªùng: `export GEMINI_API_KEY=your_key_here`"
                    )
                else:
                    # Kh·ªüi t·∫°o model m·ªôt l·∫ßn / session
                    if "gemini_model" not in st.session_state:
                        with st.spinner("ƒêang kh·ªüi t·∫°o Gemini model..."):
                            model, model_info = init_gemini(api_key)
                        if model is None:
                            suggestion_placeholder.error(f"‚ùå **L·ªói kh·ªüi t·∫°o Gemini:** {model_info}")
                        else:
                            st.session_state.gemini_model = model
                            st.session_state.gemini_model_name = model_info

                    model = st.session_state.get("gemini_model")
                    if model:
                        # CH·ªà g·ªçi Gemini khi emotion thay ƒë·ªïi
                        previous_emotion = st.session_state.previous_emotion
                        emotion_changed = previous_emotion != dominant_emotion

                        if emotion_changed or previous_emotion is None:
                            # Set flags
                            st.session_state.is_gemini_processing = True
                            st.session_state.waiting_for_ai = True

                            # G·ªçi Gemini + SQLite (memory) v√† ƒë·ª£i response (blocking) v·ªõi spinner
                            with st.spinner(f"ü§î AI tr·ª£ l√Ω c·∫£m x√∫c ƒëang suy nghƒ© v·ªÅ c·∫£m x√∫c '{dominant_emotion}'..."):
                                try:
                                    # S·ª≠ d·ª•ng agent c√≥ memory (SQLite) thay v√¨ ch·ªâ c·∫£m x√∫c hi·ªán t·∫°i
                                    suggestion_text = generate_advice_with_memory_from_result(
                                        model=model,
                                        dominant_emotion=dominant_emotion,
                                        emotions=emotions,
                                        user_id="default_user",
                                    )
                                except Exception as e:
                                    suggestion_text = f"‚ö†Ô∏è L·ªói khi g·ªçi Gemini: {str(e)}"
                                    st.error(f"‚ùå Exception: {e}")

                            # Clear flags sau khi xong
                            st.session_state.is_gemini_processing = False
                            st.session_state.waiting_for_ai = False

                            # L∆∞u emotion v√† suggestion
                            st.session_state.previous_emotion = dominant_emotion
                            st.session_state.last_gemini_suggestion = suggestion_text

                            # Hi·ªÉn th·ªã response ngay l·∫≠p t·ª©c
                            if suggestion_text and suggestion_text.strip():
                                if suggestion_text.startswith("‚ö†Ô∏è"):
                                    suggestion_placeholder.warning(suggestion_text)
                                    status_placeholder.warning("‚ö†Ô∏è C√≥ l·ªói x·∫£y ra khi g·ªçi AI")
                                else:
                                    suggestion_placeholder.markdown(
                                        f"### üí¨ G·ª£i √Ω t·ª´ tr·ª£ l√Ω c·∫£m x√∫c\n\n{suggestion_text}"
                                    )
                                    
                                    # T·∫°o v√† ph√°t audio n·∫øu TTS ƒë∆∞·ª£c b·∫≠t
                                    if tts_enabled:
                                        st.session_state.is_playing_audio = True
                                        
                                        # T·∫°o c√¢u gi·ªõi thi·ªáu c·∫£m x√∫c
                                        emotion_intro = create_emotion_intro(dominant_emotion)
                                        
                                        # N·ªëi c√¢u gi·ªõi thi·ªáu v·ªõi response t·ª´ AI
                                        full_text_to_speak = emotion_intro + suggestion_text
                                        
                                        # T·∫°o audio file
                                        with st.spinner("üîä ƒêang t·∫°o audio..."):
                                            audio_file = text_to_speech_file(full_text_to_speak, lang="vi", slow=False)
                                        
                                        if audio_file:
                                            st.session_state.current_audio_file = audio_file
                                            
                                            # Ph√°t audio trong Streamlit
                                            audio_placeholder.audio(audio_file, format="audio/mp3", autoplay=True)
                                            
                                            # ∆Ø·ªõc t√≠nh th·ªùi gian (bao g·ªìm c·∫£ intro)
                                            estimated_duration = estimate_speech_duration(full_text_to_speak)
                                            status_placeholder.info(
                                                f"üîä **ƒêang ph√°t audio...** (∆∞·ªõc t√≠nh ~{int(estimated_duration)}s). "
                                                "Sau khi ph√°t xong s·∫Ω detect c·∫£m x√∫c ti·∫øp theo."
                                            )
                                            
                                            # ƒê·ª£i audio ph√°t xong (∆∞·ªõc t√≠nh)
                                            time.sleep(estimated_duration + 1)  # +1s buffer
                                            
                                            # Cleanup audio file
                                            cleanup_audio_file(audio_file)
                                            st.session_state.is_playing_audio = False
                                            st.session_state.current_audio_file = None
                                            
                                            status_placeholder.success("‚úÖ **ƒê√£ ƒë·ªçc xong!** S·∫µn s√†ng detect c·∫£m x√∫c ti·∫øp theo.")
                                        else:
                                            st.session_state.is_playing_audio = False
                                            status_placeholder.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o audio. Ti·∫øp t·ª•c detect c·∫£m x√∫c...")
                                    else:
                                        status_placeholder.success("‚úÖ **AI ƒë√£ tr·∫£ l·ªùi xong!** S·∫µn s√†ng detect c·∫£m x√∫c ti·∫øp theo.")
                            else:
                                suggestion_placeholder.error(
                                    f"‚ùå **Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini!**\n\n"
                                    f"**Debug:** suggestion_text = `{repr(suggestion_text)}`"
                                )
                                status_placeholder.error("‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                            
                            # N·∫øu auto mode, t·ª± ƒë·ªông detect ti·∫øp sau khi c√≥ response v√† audio ph√°t xong
                            if auto_mode and suggestion_text and not suggestion_text.startswith("‚ö†Ô∏è"):
                                if not tts_enabled or not st.session_state.is_playing_audio:
                                    # N·∫øu kh√¥ng c√≥ TTS ho·∫∑c audio ƒë√£ ph√°t xong, ƒë·ª£i m·ªôt ch√∫t r·ªìi detect ti·∫øp
                                    time.sleep(1)  # ƒê·ª£i 1s ƒë·ªÉ user ƒë·ªçc response
                                    st.rerun()
                        else:
                            # Emotion kh√¥ng ƒë·ªïi, hi·ªÉn th·ªã suggestion c≈© nh∆∞ng v·∫´n ti·∫øp t·ª•c detect
                            if st.session_state.last_gemini_suggestion:
                                suggestion_placeholder.markdown(
                                    f"### üí¨ G·ª£i √Ω t·ª´ tr·ª£ l√Ω c·∫£m x√∫c\n\n{st.session_state.last_gemini_suggestion}"
                                )
                                status_placeholder.info(
                                    f"‚ÑπÔ∏è C·∫£m x√∫c '{dominant_emotion}' kh√¥ng thay ƒë·ªïi (gi·ªëng '{previous_emotion}'). "
                                    "ƒêang ti·∫øp t·ª•c capture v√† detect c·∫£m x√∫c m·ªõi..."
                                )
                            else:
                                suggestion_placeholder.info(
                                    f"‚ÑπÔ∏è C·∫£m x√∫c '{dominant_emotion}' kh√¥ng thay ƒë·ªïi. "
                                    "ƒêang ti·∫øp t·ª•c capture v√† detect c·∫£m x√∫c m·ªõi..."
                                )
                            
                            # V·∫´n c·∫≠p nh·∫≠t previous_emotion
                            st.session_state.previous_emotion = dominant_emotion
                            
                            # N·∫øu auto mode, trigger detect ti·∫øp sau interval_seconds
                            if auto_mode:
                                # Set th·ªùi gian ƒë·ªÉ detect ti·∫øp
                                st.session_state.last_detection_time = time.time() - interval_seconds + 1
                                # T·ª± ƒë·ªông rerun sau m·ªôt ch√∫t ƒë·ªÉ detect ti·∫øp
                                time.sleep(1)
                                st.rerun()
            else:
                if processor.last_frame_bgr is None:
                    result_placeholder.info(
                        "üì∑ **ƒêang ch·ªù camera...**\n\n"
                        "H√£y ƒë·∫£m b·∫£o camera ƒë√£ ƒë∆∞·ª£c b·∫≠t v√† cho ph√©p truy c·∫≠p."
                    )
                else:
                    result_placeholder.info("üí° Nh·∫•n 'Detect c·∫£m x√∫c ngay' ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch.")


