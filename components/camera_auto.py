import av
import cv2
import streamlit as st
import time
import numpy as np
from PIL import Image
from streamlit_webrtc import WebRtcMode, VideoProcessorBase, webrtc_streamer

from services.deepface_service import predict_emotion
from services.face_recognition_service import (
    detect_and_identify_faces,
    get_largest_face,
    draw_face_boxes,
    reset_face_database,
    get_face_count,
)
# upsert_emotion_memory ƒë∆∞·ª£c g·ªçi trong emotion_agent_service, kh√¥ng c·∫ßn import ·ªü ƒë√¢y
from services.gemini_service import (
    get_gemini_api_key,
    init_gemini,
    create_emotion_intro,
)
from services.emotion_agent_service import (
    generate_advice_with_memory_from_result,
)
from services.tts_service import text_to_speech_file, estimate_speech_duration, cleanup_audio_file


class EmotionVideoProcessor(VideoProcessorBase):
    """
    Video processor d√πng cho streamlit-webrtc.
    Detect faces v·ªõi YOLO v√† track face_id v·ªõi FaceNet.
    """

    def __init__(self):
        # Frame BGR m·ªõi nh·∫•t t·ª´ camera
        self.last_frame_bgr = None
        # Frame ƒë√£ ƒë∆∞·ª£c v·∫Ω annotations (faces, emotions)
        self.annotated_frame = None
        # ·∫¢nh ƒë√£ ƒë∆∞·ª£c "ch·ª•p" (PIL Image) gi·ªëng nh∆∞ Take photo
        self.captured_image = None
        # K·∫øt qu·∫£ ph√¢n t√≠ch c·∫£m x√∫c g·∫ßn nh·∫•t
        self.last_result = None
        # Danh s√°ch faces ƒë∆∞·ª£c detect trong frame hi·ªán t·∫°i
        self.detected_faces = []
        # Face ƒë∆∞·ª£c ch·ªçn (l·ªõn nh·∫•t)
        self.selected_face = None
        # Dict l∆∞u emotion cho t·ª´ng face_id {face_id: emotion_name}
        self.face_emotions = {}
        # Enable/disable real-time detection trong recv()
        self.realtime_detection = True

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        # L∆∞u frame m·ªõi nh·∫•t
        img_bgr = frame.to_ndarray(format="bgr24")
        self.last_frame_bgr = img_bgr.copy()
        
        # Real-time face detection v√† annotation
        if self.realtime_detection:
            try:
                # Detect v√† identify faces
                faces = detect_and_identify_faces(img_bgr)
                self.detected_faces = faces
                
                # Ch·ªçn face l·ªõn nh·∫•t
                largest_face = get_largest_face(faces)
                self.selected_face = largest_face
                
                # Predict emotion cho t·ª´ng face v√† c·∫≠p nh·∫≠t dict
                for face in faces:
                    face_id = face["face_id"]
                    face_img = face["face_img"]
                    
                    # Predict emotion b·∫±ng MLT model
                    emotion, prob, emotion_probs = predict_emotion(face_img)
                    self.face_emotions[face_id] = emotion
                    
                    # L∆∞u v√†o face dict ƒë·ªÉ d√πng sau
                    face["emotion"] = emotion
                    face["emotion_prob"] = prob
                    face["emotion_probs"] = emotion_probs
                
                # V·∫Ω annotations l√™n frame
                selected_id = largest_face["face_id"] if largest_face else None
                annotated = draw_face_boxes(
                    img_bgr,
                    faces,
                    selected_face_id=selected_id,
                    show_emotion=True,
                    emotions=self.face_emotions
                )
                self.annotated_frame = annotated
                
                # Tr·∫£ v·ªÅ frame ƒë√£ annotate
                return av.VideoFrame.from_ndarray(annotated, format="bgr24")
                
            except Exception as e:
                # N·∫øu l·ªói, tr·∫£ v·ªÅ frame g·ªëc
                pass
        
        return frame


def render_camera_auto(interval_seconds: int = 15):
    """
    Giao di·ªán v√† logic cho ch·∫ø ƒë·ªô Camera auto.
    Sequential flow: Detect emotion ‚Üí Call Gemini ‚Üí Show response ‚Üí Detect ti·∫øp
    T√≠ch h·ª£p YOLO face detection v√† FaceNet face recognition.
    """
    st.subheader("üçΩÔ∏è H·ªá th·ªëng h·ªó tr·ª£ ph·ª•c v·ª• kh√°ch h√†ng - Nh√† h√†ng")
    st.write(
        "**Quy tr√¨nh:** Detect khu√¥n m·∫∑t kh√°ch ‚Üí Nh·∫≠n di·ªán (Face ID) ‚Üí Ph√¢n t√≠ch c·∫£m x√∫c ‚Üí AI ƒë∆∞a h∆∞·ªõng d·∫´n cho nh√¢n vi√™n\n\n"
        "B·∫≠t camera ƒë·ªÉ theo d√µi c·∫£m x√∫c kh√°ch h√†ng v√† nh·∫≠n h∆∞·ªõng d·∫´n ph·ª•c v·ª• ph√π h·ª£p."
    )
    
    # Kh·ªüi t·∫°o session state
    if "previous_emotion" not in st.session_state:
        st.session_state.previous_emotion = {}  # Dict {face_id: emotion} thay v√¨ single value
    if "is_gemini_processing" not in st.session_state:
        st.session_state.is_gemini_processing = False
    if "last_gemini_suggestion" not in st.session_state:
        st.session_state.last_gemini_suggestion = {}  # Dict {face_id: suggestion}
    if "last_detection_time" not in st.session_state:
        st.session_state.last_detection_time = 0
    if "waiting_for_ai" not in st.session_state:
        st.session_state.waiting_for_ai = False
    if "is_playing_audio" not in st.session_state:
        st.session_state.is_playing_audio = False
    if "current_audio_file" not in st.session_state:
        st.session_state.current_audio_file = None
    if "current_face_id" not in st.session_state:
        st.session_state.current_face_id = None
    
    # Style cho n√∫t/checkbox (font 16px)
    st.markdown(
        """
        <style>
        .stMarkdown { font-size: 16px; }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # Control buttons
    col1, col2, col3, col4 = st.columns([5, 5, 5, 5], gap="small")
    with col1:
        if st.button("üîÑ Reset v√† b·∫Øt ƒë·∫ßu l·∫°i", use_container_width=True):
            st.session_state.previous_emotion = {}
            st.session_state.last_gemini_suggestion = {}
            st.session_state.waiting_for_ai = False
            st.session_state.is_gemini_processing = False
            st.session_state.last_detection_time = 0
            st.session_state.force_detect = True
            st.session_state.current_face_id = None
            # Reset face database
            reset_face_database()
            st.success("‚úÖ ƒê√£ reset! S·∫µn s√†ng detect c·∫£m x√∫c m·ªõi.")
            st.rerun()
    
    with col2:
        if st.button("‚ñ∂Ô∏è Detect c·∫£m x√∫c ngay", use_container_width=True):
            st.session_state.waiting_for_ai = False
            st.session_state.last_detection_time = 0
            st.session_state.force_detect = True
            st.rerun()
    
    with col3:
        auto_mode = st.checkbox("üîÑ T·ª± ƒë·ªông detect", value=False, key="auto_detect_mode")
    
    with col4:
        # Hi·ªÉn th·ªã s·ªë face ƒë√£ nh·∫≠n di·ªán
        face_count = get_face_count()
        st.metric("üë• Faces", face_count)
    
    # TTS settings
    tts_enabled = st.checkbox("üîä B·∫≠t ƒë·ªçc text-to-speech", value=True, key="tts_enabled")
    
    # Kh·ªüi t·∫°o force_detect flag
    if "force_detect" not in st.session_state:
        st.session_state.force_detect = False

    webrtc_ctx = webrtc_streamer(
        key=f"emotion-auto-{interval_seconds}",
        mode=WebRtcMode.SENDRECV,
        media_stream_constraints={"video": True, "audio": False},
        video_processor_factory=EmotionVideoProcessor,
        async_processing=True,
    )

    # Placeholders cho UI
    face_info_placeholder = st.empty()
    result_placeholder = st.empty()
    chart_placeholder = st.empty()
    suggestion_placeholder = st.empty()
    status_placeholder = st.empty()
    audio_placeholder = st.empty()

    if webrtc_ctx.video_processor is not None:
        processor = webrtc_ctx.video_processor

        # N·∫øu ƒëang ch·ªù AI response, ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ c≈©
        if st.session_state.is_gemini_processing or st.session_state.waiting_for_ai:
            status_placeholder.info("‚è≥ **ƒêang ch·ªù AI tr·∫£ l·ªùi...** Vui l√≤ng ƒë·ª£i.")
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ c≈© n·∫øu c√≥
            current_face_id = st.session_state.current_face_id
            if current_face_id is not None and processor.last_result:
                result = processor.last_result
                dominant_emotion = result.get("dominant_emotion")
                emotions = result.get("emotion", {})
                face_info_placeholder.info(f"üë§ **Face ID:** {current_face_id}")
                result_placeholder.success(f"**C·∫£m x√∫c ch√≠nh**: {dominant_emotion}")
                if emotions:
                    chart_placeholder.subheader("Chi ti·∫øt c√°c c·∫£m x√∫c")
                    chart_placeholder.bar_chart(emotions)
            if current_face_id is not None and current_face_id in st.session_state.last_gemini_suggestion:
                suggestion_placeholder.markdown(
                    f"### üçΩÔ∏è H∆∞·ªõng d·∫´n ph·ª•c v·ª• kh√°ch h√†ng\n\n{st.session_state.last_gemini_suggestion[current_face_id]}"
                )
        else:
            # Ki·ªÉm tra xem c√≥ n√™n detect kh√¥ng
            should_detect = (
                st.session_state.force_detect or
                auto_mode or 
                st.session_state.last_detection_time == 0 or
                (time.time() - st.session_state.last_detection_time) > interval_seconds
            )
            
            # Reset force_detect flag sau khi d√πng
            if st.session_state.force_detect:
                st.session_state.force_detect = False
            
            # Process face detection results
            if should_detect and processor.selected_face is not None:
                try:
                    selected_face = processor.selected_face
                    face_id = selected_face["face_id"]
                    st.session_state.current_face_id = face_id
                    
                    # L·∫•y th√¥ng tin emotion t·ª´ processor
                    dominant_emotion = selected_face.get("emotion", "Unknown")
                    emotion_probs = selected_face.get("emotion_probs", {})
                    
                    # L∆∞u captured image
                    if processor.annotated_frame is not None:
                        # Chuy·ªÉn BGR -> RGB cho PIL
                        frame_rgb = cv2.cvtColor(processor.annotated_frame, cv2.COLOR_BGR2RGB)
                        processor.captured_image = Image.fromarray(frame_rgb)
                    
                    # T·∫°o result dict (bao g·ªìm c·∫£ face features)
                    result = {
                        "face_id": face_id,
                        "dominant_emotion": dominant_emotion,
                        "emotion": emotion_probs,
                        "similarity": selected_face.get("similarity", 0),
                        "box": selected_face.get("box", []),
                        "face_embedding": selected_face.get("embedding"),  # 512-D vector
                    }
                    processor.last_result = result
                    
                    # L∆∞u emotion v√†o DB ƒë∆∞·ª£c th·ª±c hi·ªán trong emotion_agent_service
                    # khi g·ªçi generate_advice_with_memory_from_result()
                    
                    st.session_state.last_detection_time = time.time()
                    
                except Exception as e:
                    st.warning(f"L·ªói khi detect c·∫£m x√∫c: {e}")

        # Hi·ªÉn th·ªã ·∫£nh n·∫øu c√≥
        if processor.captured_image is not None:
            st.image(
                processor.captured_image,
                caption="·∫¢nh ƒë√£ capture (v·ªõi face annotations)",
                use_container_width=True,
            )

        # X·ª≠ l√Ω k·∫øt qu·∫£ v√† g·ªçi Gemini
        if not st.session_state.is_gemini_processing and not st.session_state.waiting_for_ai:
            result = processor.last_result
            if result:
                face_id = result.get("face_id")
                dominant_emotion = result.get("dominant_emotion")
                emotions = result.get("emotion", {})
                similarity = result.get("similarity", 0)

                face_info_placeholder.info(
                    f"üë§ **Face ID:** {face_id} | "
                    f"üìä **Similarity:** {similarity:.2f}"
                )
                result_placeholder.success(f"**C·∫£m x√∫c ch√≠nh**: {dominant_emotion}")

                if emotions:
                    chart_placeholder.subheader("Chi ti·∫øt c√°c c·∫£m x√∫c")
                    chart_placeholder.bar_chart(emotions)

                # --- G·ªçi Gemini CH·ªà KHI emotion thay ƒë·ªïi cho face_id ƒë√≥ ---
                api_key = get_gemini_api_key()
                if not api_key:
                    suggestion_placeholder.warning(
                        "‚ö†Ô∏è **Ch∆∞a t√¨m th·∫•y Gemini API key!**\n\n"
                        "Vui l√≤ng:\n"
                        "1. T·∫°o file `.env` trong th∆∞ m·ª•c g·ªëc v·ªõi n·ªôi dung: `GEMINI_API_KEY=your_key_here`\n"
                        "2. Ho·∫∑c nh·∫≠p API key ·ªü sidebar\n"
                        "3. Ho·∫∑c set bi·∫øn m√¥i tr∆∞·ªùng: `export GEMINI_API_KEY=your_key_here`"
                    )
                else:
                    # Kh·ªüi t·∫°o model m·ªôt l·∫ßn / session
                    if "gemini_model" not in st.session_state:
                        with st.spinner("ƒêang kh·ªüi t·∫°o Gemini model..."):
                            model, model_info = init_gemini(api_key)
                        if model is None:
                            suggestion_placeholder.error(f"‚ùå **L·ªói kh·ªüi t·∫°o Gemini:** {model_info}")
                        else:
                            st.session_state.gemini_model = model
                            st.session_state.gemini_model_name = model_info

                    model = st.session_state.get("gemini_model")
                    if model:
                        # L·∫•y emotion tr∆∞·ªõc ƒë√≥ c·ªßa face_id n√†y
                        previous_emotion = st.session_state.previous_emotion.get(face_id)
                        emotion_changed = previous_emotion != dominant_emotion

                        if emotion_changed or previous_emotion is None:
                            # Set flags
                            st.session_state.is_gemini_processing = True
                            st.session_state.waiting_for_ai = True

                            # G·ªçi Gemini v·ªõi face_id l√†m user_id, l∆∞u c·∫£ face features v√†o DB
                            with st.spinner(f"ü§î AI ƒëang ph√¢n t√≠ch c·∫£m x√∫c '{dominant_emotion}' cho Face ID {face_id}..."):
                                try:
                                    suggestion_text = generate_advice_with_memory_from_result(
                                        model=model,
                                        dominant_emotion=dominant_emotion,
                                        emotions=emotions,
                                        user_id=str(face_id),
                                        similarity=result.get("similarity"),
                                        face_embedding=result.get("face_embedding"),
                                        box=result.get("box"),
                                    )
                                except Exception as e:
                                    suggestion_text = f"‚ö†Ô∏è L·ªói khi g·ªçi Gemini: {str(e)}"
                                    st.error(f"‚ùå Exception: {e}")

                            # Clear flags sau khi xong
                            st.session_state.is_gemini_processing = False
                            st.session_state.waiting_for_ai = False

                            # L∆∞u emotion v√† suggestion theo face_id
                            st.session_state.previous_emotion[face_id] = dominant_emotion
                            st.session_state.last_gemini_suggestion[face_id] = suggestion_text

                            # Hi·ªÉn th·ªã response
                            if suggestion_text and suggestion_text.strip():
                                if suggestion_text.startswith("‚ö†Ô∏è"):
                                    suggestion_placeholder.warning(suggestion_text)
                                    status_placeholder.warning("‚ö†Ô∏è C√≥ l·ªói x·∫£y ra khi g·ªçi AI")
                                else:
                                    suggestion_placeholder.markdown(
                                        f"### üçΩÔ∏è H∆∞·ªõng d·∫´n ph·ª•c v·ª• - Kh√°ch #{face_id}\n\n{suggestion_text}"
                                    )
                                    
                                    # TTS
                                    if tts_enabled:
                                        st.session_state.is_playing_audio = True
                                        
                                        emotion_intro = create_emotion_intro(dominant_emotion)
                                        full_text_to_speak = emotion_intro + suggestion_text
                                        
                                        with st.spinner("üîä ƒêang t·∫°o audio..."):
                                            audio_file = text_to_speech_file(full_text_to_speak, lang="vi", slow=False)
                                        
                                        if audio_file:
                                            st.session_state.current_audio_file = audio_file
                                            audio_placeholder.audio(audio_file, format="audio/mp3", autoplay=True)
                                            
                                            estimated_duration = estimate_speech_duration(full_text_to_speak)
                                            status_placeholder.info(
                                                f"üîä **ƒêang ph√°t audio...** (∆∞·ªõc t√≠nh ~{int(estimated_duration)}s). "
                                                "Sau khi ph√°t xong s·∫Ω detect c·∫£m x√∫c ti·∫øp theo."
                                            )
                                            
                                            time.sleep(estimated_duration + 1)
                                            
                                            cleanup_audio_file(audio_file)
                                            st.session_state.is_playing_audio = False
                                            st.session_state.current_audio_file = None
                                            
                                            status_placeholder.success("‚úÖ **ƒê√£ ƒë·ªçc xong!** S·∫µn s√†ng detect c·∫£m x√∫c ti·∫øp theo.")
                                        else:
                                            st.session_state.is_playing_audio = False
                                            status_placeholder.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o audio. Ti·∫øp t·ª•c detect c·∫£m x√∫c...")
                                    else:
                                        status_placeholder.success("‚úÖ **AI ƒë√£ tr·∫£ l·ªùi xong!** S·∫µn s√†ng detect c·∫£m x√∫c ti·∫øp theo.")
                            else:
                                suggestion_placeholder.error(
                                    f"‚ùå **Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini!**\n\n"
                                    f"**Debug:** suggestion_text = `{repr(suggestion_text)}`"
                                )
                                status_placeholder.error("‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                            
                            # Auto mode - detect ti·∫øp
                            if auto_mode and suggestion_text and not suggestion_text.startswith("‚ö†Ô∏è"):
                                if not tts_enabled or not st.session_state.is_playing_audio:
                                    time.sleep(1)
                                    st.rerun()
                        else:
                            # Emotion kh√¥ng ƒë·ªïi cho face_id n√†y
                            if face_id in st.session_state.last_gemini_suggestion:
                                suggestion_placeholder.markdown(
                                    f"### üçΩÔ∏è H∆∞·ªõng d·∫´n ph·ª•c v·ª• - Kh√°ch #{face_id}\n\n{st.session_state.last_gemini_suggestion[face_id]}"
                                )
                                status_placeholder.info(
                                    f"‚ÑπÔ∏è C·∫£m x√∫c '{dominant_emotion}' c·ªßa Face ID {face_id} kh√¥ng thay ƒë·ªïi. "
                                    "ƒêang ti·∫øp t·ª•c detect..."
                                )
                            else:
                                suggestion_placeholder.info(
                                    f"‚ÑπÔ∏è C·∫£m x√∫c '{dominant_emotion}' c·ªßa Face ID {face_id}. "
                                    "ƒêang ti·∫øp t·ª•c detect..."
                                )
                            
                            st.session_state.previous_emotion[face_id] = dominant_emotion
                            
                            if auto_mode:
                                st.session_state.last_detection_time = time.time() - interval_seconds + 1
                                time.sleep(1)
                                st.rerun()
            else:
                if processor.last_frame_bgr is None:
                    result_placeholder.info(
                        "üì∑ **ƒêang ch·ªù camera...**\n\n"
                        "H√£y ƒë·∫£m b·∫£o camera ƒë√£ ƒë∆∞·ª£c b·∫≠t v√† cho ph√©p truy c·∫≠p."
                    )
                elif processor.selected_face is None:
                    result_placeholder.info(
                        "üë§ **Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t**\n\n"
                        "H√£y ƒë∆∞a khu√¥n m·∫∑t v√†o camera ƒë·ªÉ b·∫Øt ƒë·∫ßu detect."
                    )
                else:
                    result_placeholder.info("üí° Nh·∫•n 'Detect c·∫£m x√∫c ngay' ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch.")
