"""
Helper functions for working with Google Gemini and emotion-aware prompts.
Adapted to this project (Streamlit + DeepFace).
"""

from collections import Counter
from typing import List, Optional, Tuple

import google.generativeai as genai
import streamlit as st


# Map emotion -> emoji (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh cho ph√π h·ª£p)
emotion_emoji = {
    "angry": "üò†",
    "disgust": "ü§¢",
    "fear": "üò®",
    "happy": "üòÑ",
    "sad": "üò¢",
    "surprise": "üò≤",
    "neutral": "üòê",
}

# Map emotion -> Vietnamese name
emotion_vietnamese = {
    "angry": "t·ª©c gi·∫≠n",
    "disgust": "gh√™ t·ªüm",
    "fear": "s·ª£ h√£i",
    "happy": "vui v·∫ª",
    "sad": "bu·ªìn b√£",
    "surprise": "ng·∫°c nhi√™n",
    "neutral": "b√¨nh th∆∞·ªùng",
}


def create_emotion_intro(emotion: str) -> str:
    """
    T·∫°o c√¢u gi·ªõi thi·ªáu c·∫£m x√∫c b·∫±ng ti·∫øng Vi·ªát.
    
    Args:
        emotion: Emotion name (e.g., "happy", "sad")
    
    Returns:
        Vietnamese introduction sentence
    """
    emotion_vi = emotion_vietnamese.get(emotion, emotion)
    return f"Kh√°ch h√†ng ƒëang ·ªü c·∫£m x√∫c {emotion_vi}. "

def get_gemini_api_key() -> Optional[str]:
    """Get Gemini API key from various sources."""

    import os
    from pathlib import Path

    # Try loading from .env file first
    try:
        from dotenv import load_dotenv
        env_path = Path(__file__).parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)
    except ImportError:
        # python-dotenv ch∆∞a ƒë∆∞·ª£c c√†i, b·ªè qua
        pass
    except Exception:
        # C√≥ l·ªói khi load .env, b·ªè qua
        pass

    # Try secrets first (Streamlit secrets)
    try:
        api_key = st.secrets.get("GEMINI_API_KEY")
        if api_key:
            return api_key
    except Exception:
        pass

    # Try environment variable (sau khi ƒë√£ load .env)
    api_key = os.getenv("GEMINI_API_KEY")
    if api_key:
        return api_key

    # Try session state (user input)
    if "gemini_api_key" in st.session_state and st.session_state.gemini_api_key:
        return st.session_state.gemini_api_key

    return None


def init_gemini(api_key: str, model_name: Optional[str] = None):
    """Initialize Gemini API with API key and choose a sensible default model."""

    if not api_key:
        return None, "Ch∆∞a c√≥ GEMINI_API_KEY"

    try:
        genai.configure(api_key=api_key)
        models = genai.list_models()

        available_models = []
        free_tier_models = []

        for model in models:
            if "generateContent" in model.supported_generation_methods:
                model_name_clean = model.name.replace("models/", "")

                # L·ªçc b·ªõt experimental
                if "-exp" not in model_name_clean and "experimental" not in model_name_clean.lower():
                    available_models.append(model_name_clean)

                    if any(
                        ft_model in model_name_clean
                        for ft_model in ["gemini-1.5-flash", "gemini-1.5-pro"]
                    ):
                        free_tier_models.append(model_name_clean)

        preferred_models = [
            "gemini-1.5-flash",  # nhanh, free tier t·ªët
            "gemini-1.5-pro",
            
        ]

        selected_model = None

        if model_name and model_name in available_models:
            selected_model = model_name
        else:
            for pref_model in preferred_models:
                if pref_model in free_tier_models:
                    selected_model = pref_model
                    break

            if not selected_model and free_tier_models:
                selected_model = free_tier_models[0]

            if not selected_model and available_models:
                selected_model = available_models[0]

        if not selected_model:
            return None, "Kh√¥ng t√¨m th·∫•y model Gemini n√†o kh·∫£ d·ª•ng"

        model = genai.GenerativeModel(selected_model)
        return model, selected_model
    except Exception as e:
        return None, f"L·ªói khi kh·ªüi t·∫°o Gemini API: {e}"


def analyze_emotion_pattern(emotion_list: List[str]) -> Tuple[Optional[str], Optional[str]]:
    """Analyze emotion pattern from a list of emotions."""

    if not emotion_list:
        return None, None

    emotion_counts = Counter(emotion_list)
    most_common_emotion, count = emotion_counts.most_common(1)[0]
    total_count = len(emotion_list)

    parts = []
    for emotion, cnt in emotion_counts.most_common():
        pct = (cnt / total_count) * 100
        parts.append(f"{emotion}: {cnt}/{total_count} ({pct:.1f}%)")

    pattern_description = ", ".join(parts)
    return most_common_emotion, pattern_description


def generate_suggestion_for_current_emotion(
    model, current_emotion: str, max_retries: int = 3
) -> Optional[str]:
    """Generate 1 ƒëo·∫°n g·ª£i √Ω ng·∫Øn cho c·∫£m x√∫c hi·ªán t·∫°i v·ªõi retry logic cho l·ªói 429."""

    if not model or not current_emotion:
        return None

    emoji = emotion_emoji.get(current_emotion, "üòê")

    prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω c·∫£m x√∫c chuy√™n nghi·ªáp. Hi·ªán t·∫°i ng∆∞·ªùi d√πng ƒëang c√≥ c·∫£m x√∫c: {current_emotion} {emoji}

H√£y ƒë∆∞a ra m·ªôt g·ª£i √Ω h·ªó tr·ª£ ng·∫Øn g·ªçn v√† ph√π h·ª£p v·ªõi c·∫£m x√∫c hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng. Format (tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát):

**Ti√™u ƒë·ªÅ:** [Ti√™u ƒë·ªÅ ng·∫Øn g·ªçn v·ªÅ c·∫£m x√∫c {current_emotion}]

**G·ª£i √Ω:** [2-3 c√¢u g·ª£i √Ω ng·∫Øn g·ªçn, th·ª±c t·∫ø, ph√π h·ª£p v·ªõi c·∫£m x√∫c n√†y]

H√£y tr·∫£ l·ªùi ngay:"""

    import time
    import re

    # Safety settings - cho ph√©p t·∫•t c·∫£ content ƒë·ªÉ tr√°nh b·ªã block
    safety_settings = [
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_NONE",
        },
    ]
    
    for attempt in range(max_retries):
        try:
            # G·ªçi v·ªõi safety settings ƒë·ªÉ tr√°nh b·ªã block
            try:
                response = model.generate_content(
                    prompt,
                    safety_settings=safety_settings
                )
            except TypeError:
                # M·ªôt s·ªë model c√≥ th·ªÉ kh√¥ng h·ªó tr·ª£ safety_settings parameter
                response = model.generate_content(prompt)
            
            # Parse response - th·ª≠ nhi·ªÅu c√°ch
            text = None
            
            # C√°ch 1: response.text (ph·ªï bi·∫øn nh·∫•t)
            if hasattr(response, "text"):
                text = response.text
            
            # C√°ch 2: response.candidates[0].content.parts[0].text
            if not text and hasattr(response, "candidates") and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, "content"):
                    if hasattr(candidate.content, "parts") and candidate.content.parts:
                        part = candidate.content.parts[0]
                        if hasattr(part, "text"):
                            text = part.text
                    # Th·ª≠ content.text
                    if not text and hasattr(candidate.content, "text"):
                        text = candidate.content.text
                # Th·ª≠ candidate.text tr·ª±c ti·∫øp
                if not text and hasattr(candidate, "text"):
                    text = candidate.text
            
            # C√°ch 3: str(response) n·∫øu c√≥
            if not text:
                try:
                    text = str(response)
                    # N·∫øu l√† object representation, kh√¥ng d√πng
                    if text.startswith("<") and "object" in text:
                        text = None
                except:
                    pass
            
            # N·∫øu c√≥ text, return
            if text and text.strip():
                return text.strip()
            
            # N·∫øu kh√¥ng c√≥ text, return error message v·ªõi debug info
            debug_info = []
            if hasattr(response, "__dict__"):
                debug_info.append(f"response.__dict__ keys: {list(response.__dict__.keys())[:5]}")
            if hasattr(response, "candidates"):
                debug_info.append(f"candidates count: {len(response.candidates) if response.candidates else 0}")
            
            return f"""‚ö†Ô∏è **Response t·ª´ Gemini kh√¥ng c√≥ text!**

**Debug info:**
- Response type: `{type(response)}`
- Has 'text' attr: {hasattr(response, 'text')}
- Has 'candidates' attr: {hasattr(response, 'candidates')}
{chr(10).join(debug_info)}

**C√≥ th·ªÉ do:**
- Response b·ªã block b·ªüi safety settings
- Model kh√¥ng tr·∫£ v·ªÅ content
- API response format thay ƒë·ªïi

**Th·ª≠:**
- Ki·ªÉm tra API key c√≥ ƒë√∫ng kh√¥ng
- Th·ª≠ l·∫°i sau v√†i gi√¢y
- Ki·ªÉm tra quota/rate limits"""
        except Exception as e:
            error_msg = str(e)

            # X·ª≠ l√Ω l·ªói 429 (rate limit / quota exceeded)
            if "429" in error_msg or "quota" in error_msg.lower() or "exceeded" in error_msg.lower():
                # Extract retry delay n·∫øu c√≥ trong error message
                retry_delay = 5  # Default delay
                if "retry in" in error_msg.lower():
                    delay_match = re.search(r"retry in ([\d.]+)s", error_msg.lower())
                    if delay_match:
                        retry_delay = float(delay_match.group(1)) + 1

                if attempt < max_retries - 1:
                    # Exponential backoff v·ªõi max 60s
                    wait_time = min(retry_delay * (2 ** attempt), 60)
                    time.sleep(wait_time)
                    continue
                else:
                    return """‚ö†Ô∏è **ƒê√£ v∆∞·ª£t qu√° h·∫°n m·ª©c (quota) API mi·ªÖn ph√≠!**

**Nguy√™n nh√¢n:**
- B·∫°n ƒë√£ s·ª≠ d·ª•ng h·∫øt quota mi·ªÖn ph√≠ cho model hi·ªán t·∫°i
- Model ƒëang d√πng c√≥ th·ªÉ kh√¥ng c√≥ trong free tier

**Gi·∫£i ph√°p:**
1. ƒê·ª£i m·ªôt ch√∫t (th∆∞·ªùng v√†i ph√∫t ƒë·∫øn v√†i gi·ªù) ƒë·ªÉ quota reset
2. Ki·ªÉm tra usage t·∫°i: https://ai.dev/usage?tab=rate-limit
3. Xem rate limits t·∫°i: https://ai.google.dev/gemini-api/docs/rate-limits

**L∆∞u √Ω:** Model `gemini-1.5-flash` th∆∞·ªùng c√≥ quota t·ªët h∆°n cho free tier."""

            # X·ª≠ l√Ω l·ªói 404 (model not found)
            elif "404" in error_msg or "not found" in error_msg.lower():
                return "‚ö†Ô∏è L·ªói: Model kh√¥ng t√¨m th·∫•y. Vui l√≤ng ki·ªÉm tra API key v√† model name."

            # C√°c l·ªói kh√°c
            elif attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                return f"‚ö†Ô∏è Xin l·ªói, c√≥ l·ªói x·∫£y ra sau {max_retries} l·∫ßn th·ª≠: {error_msg[:200]}"

    return "‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi Gemini API. Vui l√≤ng th·ª≠ l·∫°i sau."


